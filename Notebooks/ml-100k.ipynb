{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to look at\n",
    "\n",
    "Some lines may be out of order. Please re-order them.\n",
    "\n",
    "\n",
    "- **Labels :** I do not really know how to form correct labels. As of now I run \n",
    "```python\n",
    "num_samples = values.shape[0]\n",
    "labels = torch.randint(low=0, high=2, size=(num_samples,), dtype=torch.float32)\n",
    "```\n",
    "to generate labels which are simply random 0,1s but of shape that was compatible with GNN.\n",
    "I did not understand how to get this.\n",
    "if you look at examples/example.py for `Bail` and print you will get :\n",
    "```bash\n",
    "torch.Size([18876, 18876]) torch.Size([18876, 18]) torch.Size([18876])\n",
    "```\n",
    "\n",
    "- **Train/test/val splits:** I'm not sure if this is correct. In Bail() they did it after getting the labels.\n",
    "This can be seen as a commented block taken from `Bail()` loading function for generating test_train_val splits. However, this might be different as well. But Bail() does seem to have 0/1 labels taken from the bail dataset.\n",
    "\n",
    "- I'm not too sure what the fuck they are doing with `user_sens` or `sens_idx', maybe they just take it to use to get the group-based fairness scores we are trying to calculate (99% sure)\n",
    "\n",
    "-> I think they're using sens_idx to keep track of the different sensitive groups? user_sens should be the sensitive attributes, e.g. 0 or 1 for gender\n",
    "\n",
    "- **features:** The graph is bipartite so we cannot just add only user-features, since we need to maintain shapes, we also have to add movie-features. I add `torch.zeros` as movie features as of now.\n",
    "\n",
    "\n",
    "**Note: The shapes should be of form**\n",
    "```python\n",
    "print(adj.shape,feats.shape,labels.shape)\n",
    "```\n",
    "$N \\times N$; $N \\times D$; $N$ where D are D dimensional features. In our case since I use age and occupation its 2 as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.item()` if only 1 tensor([1.123]) or\n",
    "use `.detach().cpu().numpy()` if you want to convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m      6\u001b[0m     random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m----> 9\u001b[0m \u001b[43msetup_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36msetup_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_seed\u001b[39m(seed):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygdebias/lib/python3.9/site-packages/torch/random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
      "File \u001b[0;32m~/miniconda3/envs/pygdebias/lib/python3.9/site-packages/torch/cuda/random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    110\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    111\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 113\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pygdebias/lib/python3.9/site-packages/torch/cuda/__init__.py:156\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/miniconda3/envs/pygdebias/lib/python3.9/site-packages/torch/cuda/random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    110\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "setup_seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1682])\n",
      "torch.Size([943])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "from dataset import Ml_100k\n",
    "#from ..pygdebias.datasets import Ml_100k\n",
    "import scipy.sparse as sp\n",
    "\n",
    "ml100k = Ml_100k()\n",
    "rating_matrix,user_sens=ml100k.rating_matrix ,ml100k.user_sens\n",
    "\n",
    "print(rating_matrix[0].shape)\n",
    "\n",
    "print(user_sens.shape)\n",
    "\n",
    "\n",
    "\n",
    "user_num = user_sens.shape[0]\n",
    "item_num = rating_matrix[0].shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([943, 1682])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm=rating_matrix.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_r=rm.reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero=np.argwhere(rm>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm=rating_matrix.detach().numpy()\n",
    "l_r=rm.reshape([-1])\n",
    "non_zero=np.argwhere(rm>0)\n",
    "labels=rating_matrix[non_zero[:, 0], non_zero[:, 1]]\n",
    "labels[labels<4]=0\n",
    "labels[labels>=4]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1.,  ..., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 3., 4.,  ..., 3., 3., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix[non_zero[:, 0], non_zero[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_data(rating_matrix, user_sens):\n",
    "    \"\"\"\n",
    "    Save the rating matrix and user_sens into separate numpy array files.\n",
    "    \n",
    "    Args:\n",
    "        rating_matrix (np.ndarray): The user-item rating matrix.\n",
    "        user_sens (np.ndarray): The user sensitivity array.\n",
    "    \"\"\"\n",
    "    np.save('rating_matrix.npy', rating_matrix)\n",
    "    np.save('user_sens.npy', user_sens)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the rating matrix and user_sens from the saved numpy array files.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The user-item rating matrix.\n",
    "        np.ndarray: The user sensitivity array.\n",
    "    \"\"\"\n",
    "    rating_matrix = np.load('rating_matrix.npy')\n",
    "    user_sens = np.load('user_sens.npy')\n",
    "    return torch.tensor(rating_matrix), torch.tensor(user_sens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_data(rating_matrix,user_sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm,uss=load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Train/Test Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(user_num)\n",
    "np.random.shuffle(idx)\n",
    "num_train = int(0.5 * len(idx))\n",
    "num_val = int(0.25 * len(idx))\n",
    "idx_train = torch.LongTensor(idx[:num_train])\n",
    "idx_val = torch.LongTensor(idx[num_train:num_train + num_val])\n",
    "idx_test = torch.LongTensor(idx[num_train + num_val:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # label_idx_0 = np.where(labels == 0)[0]\n",
    "        # label_idx_1 = np.where(labels == 1)[0]\n",
    "        # random.shuffle(label_idx_0)\n",
    "        # random.shuffle(label_idx_1)\n",
    "        # idx_train = np.append(\n",
    "        #     label_idx_0[: min(int(0.5 * len(label_idx_0)), label_number // 2)],\n",
    "        #     label_idx_1[: min(int(0.5 * len(label_idx_1)), label_number // 2)],\n",
    "        # )\n",
    "        # idx_val = np.append(\n",
    "        #     label_idx_0[int(0.5 * len(label_idx_0)) : int(0.75 * len(label_idx_0))],\n",
    "        #     label_idx_1[int(0.5 * len(label_idx_1)) : int(0.75 * len(label_idx_1))],\n",
    "        # )\n",
    "        # idx_test = np.append(\n",
    "        #     label_idx_0[int(0.75 * len(label_idx_0)) :],\n",
    "        #     label_idx_1[int(0.75 * len(label_idx_1)) :],\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens=user_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = np.where(rating_matrix> 0)\n",
    "values = np.ones_like(rows) # Indicate edges with 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacencey Matrix Definition\n",
    "Its a bit wierd because it's Bi-partite.\n",
    "Take a look here: https://en.wikipedia.org/wiki/Adjacency_matrix#Adjacency_matrix_of_a_bipartite_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sp.coo_matrix((values, (rows, cols + user_num)), shape=(user_num + item_num, user_num + item_num))\n",
    "adj = torch.sparse_coo_tensor(torch.LongTensor(np.vstack([adj.row, adj.col])), torch.FloatTensor(adj.data),\n",
    "                                           torch.Size(adj.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows_1,cols_1=np.where(rating_matrix>= 3)\n",
    "#labels = np.zeros_like(rating_matrix)\n",
    "#labels[rows_1, cols_1] = 1 # Recommendation for ratings above 3, otherwise no recommendation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are not accurate currently, just random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3., 4.,  ..., 0., 0., 0.],\n",
       "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 5., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = values.shape[0]\n",
    "labels = torch.randint(low=0, high=2, size=(num_samples,), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = torch.tensor(np.column_stack([rows, cols + user_num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_idx=np.where(ml100k.user_sens==1)[0] # Gender based at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = open(\"dataset/ml-100k/\"+\"u.user\")\n",
    "user_feat = []\n",
    "for line in user_info:\n",
    "    infor = line.strip().split(\"|\")\n",
    "    user_feat.append(infor[1:])\n",
    "\n",
    "user_feat=pd.DataFrame(data=user_feat,columns=[\"age\",\"gender\",\"occupation\",\"zipcode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "- feats: useless features (they run)\n",
    "- feats: Concatenated (age,occupation + item) features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice shape of each matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2625, 2625]) torch.Size([100000, 2]) torch.Size([100000]) torch.Size([2625])\n"
     ]
    }
   ],
   "source": [
    "print(adj.shape, edges.shape, labels.shape, feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Bi-partitite graph so the way feature-matrix works at least what I understood is that\n",
    "there are both user and item features. So I added random features for items as of now.\n",
    "I think it needs to be of shape [2625,D] where D is the number of features for each item (maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occupation Encoded Matrix Data Type: int64\n",
      "Combined Features Data Type: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "dataset_path = \"./dataset/ml-100k\"\n",
    "user_file_path = os.path.join(dataset_path, \"u.user\")\n",
    "\n",
    "# Read user data into a DataFrame\n",
    "user_data = pd.read_csv(user_file_path, sep='|', names=[\"user_id\", \"age\", \"gender\", \"occupation\", \"zip_code\"])\n",
    "\n",
    "# Extract relevant columns: Age and Occupation\n",
    "# Ensure ages are numeric by converting to float explicitly\n",
    "ages = pd.to_numeric(user_data['age'], errors='coerce').values.reshape(-1, 1)\n",
    "\n",
    "# Use a label encoder for the occupation\n",
    "occupations = user_data['occupation']\n",
    "encoder = LabelEncoder() # OneHotEncoder()\n",
    "occupations_encoded = encoder.fit_transform(occupations).reshape(-1, 1)\n",
    "\n",
    "# Verify the resulting encoding matrix is purely numeric\n",
    "print(\"Occupation Encoded Matrix Data Type:\", occupations_encoded.dtype)\n",
    "\n",
    "# Combine ages and encoded occupations\n",
    "features_np = np.hstack([ages, occupations_encoded])\n",
    "\n",
    "# Check if any data type is still problematic\n",
    "print(\"Combined Features Data Type:\", features_np.dtype)\n",
    "\n",
    "# Convert to a PyTorch tensor, ensuring a compatible data type\n",
    "user_features = torch.tensor(features_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the unique occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(occupations_encoded.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.nn import Embedding\n",
    "#Embedding()\n",
    "# Recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 943\n",
    "#user_features = torch.tensor(np.random.randn(num_users, 2), dtype=torch.float32)\n",
    "\n",
    "# Item features (initialize with random vectors to match the same number of columns as user features)\n",
    "num_items = 1682\n",
    "num_columns = user_features.shape[1]\n",
    "item_features = torch.zeros((num_items,num_columns),dtype=torch.float32)\n",
    "\n",
    "# Concatenate user and item features\n",
    "feats = torch.cat([user_features, item_features], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2625, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Code\n",
    "\n",
    "This code was basically copy-pasted entirely from\n",
    " //pygdebias.debiasing.GNN.py\n",
    "this was done because of many suddeny import failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv,\n",
    "    GATConv,\n",
    "    GINConv,\n",
    "    SAGEConv,\n",
    "    DeepGraphInfomax,\n",
    "    JumpingKnowledge,\n",
    ")\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch_geometric.utils import dropout_adj, convert\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "trainL=[]\n",
    "valL=[]\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, ft_in, nb_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # Classifier projector\n",
    "        self.fc1 = spectral_norm(nn.Linear(ft_in, nb_classes))\n",
    "\n",
    "    def forward(self, seq):\n",
    "        ret = self.fc1(seq)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GCNConv(nfeat, nhid)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gc1(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout=0.5):\n",
    "        super(GIN, self).__init__()\n",
    "\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(nfeat, nhid)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(nhid),\n",
    "            spectral_norm(nn.Linear(nhid, nhid)),\n",
    "        )\n",
    "        self.conv1 = GINConv(self.mlp1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class JK(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout=0.5):\n",
    "        super(JK, self).__init__()\n",
    "        self.conv1 = spectral_norm(GCNConv(nfeat, nhid))\n",
    "        self.convx = spectral_norm(GCNConv(nhid, nhid))\n",
    "        self.jk = JumpingKnowledge(mode=\"max\")\n",
    "        self.transition = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        xs = []\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.transition(x)\n",
    "        xs.append(x)\n",
    "        for _ in range(1):\n",
    "            x = self.convx(x, edge_index)\n",
    "            x = self.transition(x)\n",
    "            xs.append(x)\n",
    "        x = self.jk(xs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout=0.5):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        # Implemented spectral_norm in the sage main file\n",
    "        # ~/anaconda3/envs/PYTORCH/lib/python3.7/site-packages/torch_geometric/nn/conv/sage_conv.py\n",
    "        self.conv1 = SAGEConv(nfeat, nhid, normalize=True)\n",
    "        self.conv1.aggr = \"mean\"\n",
    "        self.transition = nn.Sequential(\n",
    "            nn.ReLU(), nn.BatchNorm1d(nhid), nn.Dropout(p=dropout)\n",
    "        )\n",
    "        self.conv2 = SAGEConv(nhid, nhid, normalize=True)\n",
    "        self.conv2.aggr = \"mean\"\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.transition(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder_DGI(nn.Module):\n",
    "    def __init__(self, nfeat, nhid):\n",
    "        super(Encoder_DGI, self).__init__()\n",
    "        self.hidden_ch = nhid\n",
    "        self.conv = spectral_norm(GCNConv(nfeat, self.hidden_ch))\n",
    "        self.activation = nn.PReLU()\n",
    "\n",
    "    def corruption(self, x, edge_index):\n",
    "        # corrupted features are obtained by row-wise shuffling of the original features\n",
    "        # corrupted graph consists of the same nodes but located in different places\n",
    "        return x[torch.randperm(x.size(0))], edge_index\n",
    "\n",
    "    def summary(self, z, *args, **kwargs):\n",
    "        return torch.sigmoid(z.mean(dim=0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphInfoMax(nn.Module):\n",
    "    def __init__(self, enc_dgi):\n",
    "        super(GraphInfoMax, self).__init__()\n",
    "        self.dgi_model = DeepGraphInfomax(\n",
    "            enc_dgi.hidden_ch, enc_dgi, enc_dgi.summary, enc_dgi.corruption\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        pos_z, neg_z, summary = self.dgi_model(x, edge_index)\n",
    "        return pos_z\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, base_model=\"gcn\", k: int = 2\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        if self.base_model == \"gcn\":\n",
    "            self.conv = GCN(in_channels, out_channels)\n",
    "        elif self.base_model == \"gin\":\n",
    "            self.conv = GIN(in_channels, out_channels)\n",
    "        elif self.base_model == \"sage\":\n",
    "            self.conv = SAGE(in_channels, out_channels)\n",
    "        elif self.base_model == \"infomax\":\n",
    "            enc_dgi = Encoder_DGI(nfeat=in_channels, nhid=out_channels)\n",
    "            self.conv = GraphInfoMax(enc_dgi=enc_dgi)\n",
    "        elif self.base_model == \"jk\":\n",
    "            self.conv = JK(in_channels, out_channels)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n",
    "        x = self.conv(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        adj,\n",
    "        features,\n",
    "        labels,\n",
    "        idx_train,\n",
    "        idx_val,\n",
    "        idx_test,\n",
    "        sens,\n",
    "        sens_idx,\n",
    "        num_hidden=16,\n",
    "        num_proj_hidden=16,\n",
    "        lr=0.001,\n",
    "        weight_decay=1e-5,\n",
    "        drop_edge_rate_1=0.1,\n",
    "        drop_edge_rate_2=0.1,\n",
    "        drop_feature_rate_1=0.1,\n",
    "        drop_feature_rate_2=0.1,\n",
    "        encoder=\"gcn\",\n",
    "        sim_coeff=0.5,\n",
    "        nclass=1,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # self.edge_index = convert.from_scipy_sparse_matrix(sp.coo_matrix(adj.to_dense().numpy()))[0]\n",
    "        self.edge_index = adj.coalesce().indices()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            in_channels=features.shape[1], out_channels=num_hidden, base_model=encoder\n",
    "        ).to(device)\n",
    "        # model = SSF(encoder=encoder, num_hidden=args.hidden, num_proj_hidden=args.proj_hidden, sim_coeff=args.sim_coeff,\n",
    "        # nclass=num_class).to(device)\n",
    "\n",
    "        self.sim_coeff = sim_coeff\n",
    "        # self.encoder = encoder\n",
    "        self.labels = labels\n",
    "\n",
    "        self.idx_train = idx_train\n",
    "        self.idx_val = idx_val\n",
    "        self.idx_test = idx_test\n",
    "        self.sens = sens\n",
    "        self.sens_idx = sens_idx\n",
    "        self.drop_edge_rate_1 = self.drop_edge_rate_2 = 0\n",
    "        self.drop_feature_rate_1 = self.drop_feature_rate_2 = 0\n",
    "\n",
    "        # Projection\n",
    "        self.fc1 = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(num_hidden, num_proj_hidden)),\n",
    "            nn.BatchNorm1d(num_proj_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(num_proj_hidden, num_hidden)),\n",
    "            nn.BatchNorm1d(num_hidden),\n",
    "        )\n",
    "\n",
    "        # Prediction\n",
    "        self.fc3 = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(num_hidden, num_hidden)),\n",
    "            nn.BatchNorm1d(num_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc4 = spectral_norm(nn.Linear(num_hidden, num_hidden))\n",
    "\n",
    "        # Classifier\n",
    "        self.c1 = Classifier(ft_in=num_hidden, nb_classes=nclass)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "        par_1 = (\n",
    "            list(self.encoder.parameters())\n",
    "            + list(self.fc1.parameters())\n",
    "            + list(self.fc2.parameters())\n",
    "            + list(self.fc3.parameters())\n",
    "            + list(self.fc4.parameters())\n",
    "        )\n",
    "        par_2 = list(self.c1.parameters()) + list(self.encoder.parameters())\n",
    "        self.optimizer_1 = optim.Adam(par_1, lr=lr, weight_decay=weight_decay)\n",
    "        self.optimizer_2 = optim.Adam(par_2, lr=lr, weight_decay=weight_decay)\n",
    "        self = self.to(device)\n",
    "\n",
    "        self.features = features.to(device)\n",
    "        self.edge_index = self.edge_index.to(device)\n",
    "        self.labels = self.labels.to(device)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x, edge_index)\n",
    "\n",
    "    def projection(self, z):\n",
    "        z = self.fc1(z)\n",
    "        z = self.fc2(z)\n",
    "        return z\n",
    "\n",
    "    def prediction(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.fc4(z)\n",
    "        return z\n",
    "\n",
    "    def classifier(self, z):\n",
    "        return self.c1(z)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        val = torch.norm(x, p=2, dim=1).detach()\n",
    "        x = x.div(val.unsqueeze(dim=1).expand_as(x))\n",
    "        return x\n",
    "\n",
    "    def D_entropy(self, x1, x2):\n",
    "        x2 = x2.detach()\n",
    "        return (\n",
    "            -torch.max(F.softmax(x2), dim=1)[0]\n",
    "            * torch.log(torch.max(F.softmax(x1), dim=1)[0])\n",
    "        ).mean()\n",
    "\n",
    "    def D(self, x1, x2):  # negative cosine similarity\n",
    "        return -F.cosine_similarity(x1, x2.detach(), dim=-1).mean()\n",
    "\n",
    "    def loss(self, z1: torch.Tensor, z2: torch.Tensor, z3: torch.Tensor, e_1, e_2, idx):\n",
    "\n",
    "        # projector\n",
    "        p1 = self.projection(z1)\n",
    "        p2 = self.projection(z2)\n",
    "\n",
    "        # predictor\n",
    "        h1 = self.prediction(p1)\n",
    "        h2 = self.prediction(p2)\n",
    "\n",
    "        # classifier\n",
    "        c1 = self.classifier(z1)\n",
    "\n",
    "        l1 = self.D(h1[idx], p2[idx]) / 2\n",
    "        l2 = self.D(h2[idx], p1[idx]) / 2\n",
    "        l3 = F.cross_entropy(c1[idx], z3[idx].squeeze().long().detach())\n",
    "\n",
    "        return self.sim_coeff * (l1 + l2), l3\n",
    "\n",
    "    def forwarding_predict(self, emb):\n",
    "\n",
    "        # classifier\n",
    "        c1 = self.classifier(emb)\n",
    "\n",
    "        return c1\n",
    "\n",
    "    def fit(self, epochs=300):\n",
    "        best_loss = 100\n",
    "        for epoch in range(epochs + 1):\n",
    "            sim_loss = 0\n",
    "\n",
    "            self.train()\n",
    "            self.optimizer_2.zero_grad()\n",
    "            edge_index_1 = self.edge_index\n",
    "            x_1 = self.features\n",
    "\n",
    "            # classifier\n",
    "            z1 = self.forward(x_1, edge_index_1)\n",
    "            c1 = self.classifier(z1)\n",
    "\n",
    "            # Binary Cross-Entropy\n",
    "            cl_loss = F.binary_cross_entropy_with_logits(\n",
    "                c1[self.idx_train],\n",
    "                self.labels[self.idx_train].unsqueeze(1).float().to(self.device),\n",
    "            )\n",
    "\n",
    "            cl_loss.backward()\n",
    "            self.optimizer_2.step()\n",
    "\n",
    "            # Validation\n",
    "            self.eval()\n",
    "            z_val = self.forward(self.features, self.edge_index)\n",
    "            c_val = self.classifier(z_val)\n",
    "            val_loss = F.binary_cross_entropy_with_logits(\n",
    "                c_val[self.idx_val],\n",
    "                self.labels[self.idx_val].unsqueeze(1).float().to(self.device),\n",
    "            )\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                trainL.append(cl_loss.item())\n",
    "                valL.append(val_loss.item())\n",
    "                print(f\"[Train] Epoch {epoch}: train_c_loss: {cl_loss:.4f} | val_c_loss: {val_loss:.4f}\")\n",
    "\n",
    "            if (val_loss) < best_loss:\n",
    "                self.val_loss = val_loss.item()\n",
    "\n",
    "                best_loss = val_loss\n",
    "                if not os.path.exists(\"data\"):\n",
    "                    os.makedirs(\"data\")\n",
    "                torch.save(self.state_dict(), f\"./data/weights_GNN_sage.pt\")\n",
    "\n",
    "    def predict(self):\n",
    "\n",
    "        self.load_state_dict(torch.load(f\"./data/weights_GNN_sage.pt\"))\n",
    "        self.eval()\n",
    "        emb = self.forward(\n",
    "            self.features.to(self.device), self.edge_index.to(self.device)\n",
    "        )\n",
    "        output = self.forwarding_predict(emb)\n",
    "\n",
    "        output_preds = (\n",
    "            (output.squeeze() > 0)\n",
    "            .type_as(self.labels)[self.idx_test]\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        labels = self.labels.detach().cpu().numpy()\n",
    "        idx_test = self.idx_test\n",
    "\n",
    "        F1 = f1_score(labels[idx_test], output_preds, average=\"micro\")\n",
    "        ACC = accuracy_score(\n",
    "            labels[idx_test],\n",
    "            output_preds,\n",
    "        )\n",
    "        try:\n",
    "            AUCROC = roc_auc_score(labels[idx_test], output_preds)\n",
    "        except:\n",
    "            AUCROC = \"nan\"\n",
    "\n",
    "        ACC_sens0, AUCROC_sens0, F1_sens0, ACC_sens1, AUCROC_sens1, F1_sens1 = (\n",
    "            self.predict_sens_group(output_preds, idx_test)\n",
    "        )\n",
    "\n",
    "        SP, EO = self.fair_metric(\n",
    "            output_preds,\n",
    "            self.labels[idx_test].detach().cpu().numpy(),\n",
    "            self.sens[idx_test].detach().cpu().numpy(),\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            ACC,\n",
    "            AUCROC,\n",
    "            F1,\n",
    "            ACC_sens0,\n",
    "            AUCROC_sens0,\n",
    "            F1_sens0,\n",
    "            ACC_sens1,\n",
    "            AUCROC_sens1,\n",
    "            F1_sens1,\n",
    "            SP,\n",
    "            EO,\n",
    "        )\n",
    "\n",
    "    def fair_metric(self, pred, labels, sens):\n",
    "\n",
    "        idx_s0 = sens == 0\n",
    "        idx_s1 = sens == 1\n",
    "        idx_s0_y1 = np.bitwise_and(idx_s0, labels == 1)\n",
    "        idx_s1_y1 = np.bitwise_and(idx_s1, labels == 1)\n",
    "        parity = abs(sum(pred[idx_s0]) / sum(idx_s0) - sum(pred[idx_s1]) / sum(idx_s1))\n",
    "        equality = abs(\n",
    "            sum(pred[idx_s0_y1]) / sum(idx_s0_y1)\n",
    "            - sum(pred[idx_s1_y1]) / sum(idx_s1_y1)\n",
    "        )\n",
    "        return parity.item(), equality.item()\n",
    "\n",
    "    def predict_sens_group(self, output, idx_test):\n",
    "        # pred = self.lgreg.predict(self.embs[idx_test])\n",
    "        pred = output\n",
    "        result = []\n",
    "        for sens in [0, 1]:\n",
    "            F1 = f1_score(\n",
    "                self.labels[idx_test][self.sens[idx_test] == sens]\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy(),\n",
    "                pred[self.sens[idx_test] == sens],\n",
    "                average=\"micro\",\n",
    "            )\n",
    "            ACC = accuracy_score(\n",
    "                self.labels[idx_test][self.sens[idx_test] == sens]\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy(),\n",
    "                pred[self.sens[idx_test] == sens],\n",
    "            )\n",
    "            try:\n",
    "                AUCROC = roc_auc_score(\n",
    "                    self.labels[idx_test][self.sens[idx_test] == sens]\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy(),\n",
    "                    pred[self.sens[idx_test] == sens],\n",
    "                )\n",
    "            except:\n",
    "                AUCROC = \"nan\"\n",
    "            result.extend([ACC, AUCROC, F1])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def drop_feature(x, drop_prob, sens_idx, sens_flag=True):\n",
    "    drop_mask = (\n",
    "        torch.empty((x.size(1),), dtype=torch.float32, device=x.device).uniform_(0, 1)\n",
    "        < drop_prob\n",
    "    )\n",
    "\n",
    "    x = x.clone()\n",
    "    drop_mask[sens_idx] = False\n",
    "\n",
    "    x[:, drop_mask] += torch.ones(1).normal_(0, 1).to(x.device)\n",
    "\n",
    "    # Flip sensitive attribute\n",
    "    if sens_flag:\n",
    "        x[:, sens_idx] = 1 - x[:, sens_idx]\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the training is being done in fit() method.\n",
    "Remove cuda() if you want to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 0: train_c_loss: 2.2047 | val_c_loss: 2.1555\n",
      "[Train] Epoch 10: train_c_loss: 1.4873 | val_c_loss: 1.4401\n",
      "[Train] Epoch 20: train_c_loss: 0.8971 | val_c_loss: 0.8711\n",
      "[Train] Epoch 30: train_c_loss: 0.6904 | val_c_loss: 0.7051\n",
      "[Train] Epoch 40: train_c_loss: 0.7230 | val_c_loss: 0.7317\n",
      "[Train] Epoch 50: train_c_loss: 0.6910 | val_c_loss: 0.6970\n",
      "[Train] Epoch 60: train_c_loss: 0.6868 | val_c_loss: 0.6934\n",
      "[Train] Epoch 70: train_c_loss: 0.6854 | val_c_loss: 0.6894\n",
      "[Train] Epoch 80: train_c_loss: 0.6835 | val_c_loss: 0.6865\n",
      "[Train] Epoch 90: train_c_loss: 0.6834 | val_c_loss: 0.6853\n",
      "[Train] Epoch 100: train_c_loss: 0.6831 | val_c_loss: 0.6844\n",
      "[Train] Epoch 110: train_c_loss: 0.6831 | val_c_loss: 0.6839\n",
      "[Train] Epoch 120: train_c_loss: 0.6830 | val_c_loss: 0.6835\n",
      "[Train] Epoch 130: train_c_loss: 0.6830 | val_c_loss: 0.6834\n",
      "[Train] Epoch 140: train_c_loss: 0.6830 | val_c_loss: 0.6833\n",
      "[Train] Epoch 150: train_c_loss: 0.6830 | val_c_loss: 0.6832\n",
      "[Train] Epoch 160: train_c_loss: 0.6830 | val_c_loss: 0.6832\n",
      "[Train] Epoch 170: train_c_loss: 0.6829 | val_c_loss: 0.6832\n",
      "[Train] Epoch 180: train_c_loss: 0.6829 | val_c_loss: 0.6832\n",
      "[Train] Epoch 190: train_c_loss: 0.6829 | val_c_loss: 0.6832\n",
      "[Train] Epoch 200: train_c_loss: 0.6829 | val_c_loss: 0.6832\n",
      "[Train] Epoch 210: train_c_loss: 0.6829 | val_c_loss: 0.6832\n",
      "[Train] Epoch 220: train_c_loss: 0.6829 | val_c_loss: 0.6832\n",
      "[Train] Epoch 230: train_c_loss: 0.6828 | val_c_loss: 0.6832\n",
      "[Train] Epoch 240: train_c_loss: 0.6828 | val_c_loss: 0.6832\n",
      "[Train] Epoch 250: train_c_loss: 0.6828 | val_c_loss: 0.6832\n",
      "[Train] Epoch 260: train_c_loss: 0.6828 | val_c_loss: 0.6832\n",
      "[Train] Epoch 270: train_c_loss: 0.6828 | val_c_loss: 0.6832\n",
      "[Train] Epoch 280: train_c_loss: 0.6828 | val_c_loss: 0.6832\n",
      "[Train] Epoch 290: train_c_loss: 0.6827 | val_c_loss: 0.6832\n",
      "[Train] Epoch 300: train_c_loss: 0.6827 | val_c_loss: 0.6832\n",
      "[Train] Epoch 310: train_c_loss: 0.6827 | val_c_loss: 0.6832\n",
      "[Train] Epoch 320: train_c_loss: 0.6827 | val_c_loss: 0.6832\n",
      "[Train] Epoch 330: train_c_loss: 0.6827 | val_c_loss: 0.6832\n",
      "[Train] Epoch 340: train_c_loss: 0.6826 | val_c_loss: 0.6832\n",
      "[Train] Epoch 350: train_c_loss: 0.6826 | val_c_loss: 0.6832\n",
      "[Train] Epoch 360: train_c_loss: 0.6826 | val_c_loss: 0.6832\n",
      "[Train] Epoch 370: train_c_loss: 0.6826 | val_c_loss: 0.6832\n",
      "[Train] Epoch 380: train_c_loss: 0.6826 | val_c_loss: 0.6832\n",
      "[Train] Epoch 390: train_c_loss: 0.6825 | val_c_loss: 0.6832\n",
      "[Train] Epoch 400: train_c_loss: 0.6825 | val_c_loss: 0.6832\n",
      "[Train] Epoch 410: train_c_loss: 0.6825 | val_c_loss: 0.6832\n",
      "[Train] Epoch 420: train_c_loss: 0.6825 | val_c_loss: 0.6832\n",
      "[Train] Epoch 430: train_c_loss: 0.6825 | val_c_loss: 0.6832\n",
      "[Train] Epoch 440: train_c_loss: 0.6824 | val_c_loss: 0.6832\n",
      "[Train] Epoch 450: train_c_loss: 0.6824 | val_c_loss: 0.6832\n",
      "[Train] Epoch 460: train_c_loss: 0.6824 | val_c_loss: 0.6832\n",
      "[Train] Epoch 470: train_c_loss: 0.6824 | val_c_loss: 0.6832\n",
      "[Train] Epoch 480: train_c_loss: 0.6824 | val_c_loss: 0.6832\n",
      "[Train] Epoch 490: train_c_loss: 0.6823 | val_c_loss: 0.6832\n",
      "[Train] Epoch 500: train_c_loss: 0.6823 | val_c_loss: 0.6832\n"
     ]
    }
   ],
   "source": [
    "# Create the device object\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "model = GNN(\n",
    "    adj,\n",
    "    feats,\n",
    "    labels,\n",
    "    idx_train,\n",
    "    idx_val,\n",
    "    idx_test,\n",
    "    sens,\n",
    "    sens_idx,\n",
    "    drop_feature_rate_1=0,\n",
    "    drop_feature_rate_2=0,\n",
    "    num_hidden=8,\n",
    "    num_proj_hidden=8\n",
    ")\n",
    "# Train the model\n",
    "model.fit(epochs=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: I have absolutely no-clue wht the code gives ACC and all that as 1 and the rest as 0. Seems super strange not gonna lie. Some of them are 1 and some of them are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(trainL,label=\"Train loss\")\n",
    "plt.plot(valL,label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict method invokes `fair_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.5485232067510548\n",
      "AUCROC:  0.5\n",
      "F1:  0.5485232067510548\n",
      "ACC_sens0: 0.581081081081081\n",
      "AUCROC_sens0:  0.5\n",
      "F1_sens0:  0.581081081081081\n",
      "ACC_sens1:  0.5337423312883436\n",
      "AUCROC_sens1:  0.5\n",
      "F1_sens1:  0.5337423312883436\n",
      "SP:  0.0\n",
      "EO: 0.0\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    ACC,\n",
    "    AUCROC,\n",
    "    F1,\n",
    "    ACC_sens0,\n",
    "    AUCROC_sens0,\n",
    "    F1_sens0,\n",
    "    ACC_sens1,\n",
    "    AUCROC_sens1,\n",
    "    F1_sens1,\n",
    "    SP,\n",
    "    EO,\n",
    ") = model.predict()\n",
    "\n",
    "print(\"ACC:\", ACC)\n",
    "print(\"AUCROC: \", AUCROC)\n",
    "print(\"F1: \", F1)\n",
    "print(\"ACC_sens0:\", ACC_sens0)\n",
    "print(\"AUCROC_sens0: \", AUCROC_sens0)\n",
    "print(\"F1_sens0: \", F1_sens0)\n",
    "print(\"ACC_sens1: \", ACC_sens1)\n",
    "print(\"AUCROC_sens1: \", AUCROC_sens1)\n",
    "print(\"F1_sens1: \", F1_sens1)\n",
    "print(\"SP: \", SP)\n",
    "print(\"EO:\", EO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score, Acc Score. Group-based scores are denoted by METRIC_sens0/1. SP and EO are parity and equal odds "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygdebias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
